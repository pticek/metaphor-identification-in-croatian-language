{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbqyY_f078g4"
   },
   "source": [
    "# Utilis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqUsNy6P8XJU"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-30T17:59:26.364599Z",
     "iopub.status.busy": "2025-05-30T17:59:26.364276Z",
     "iopub.status.idle": "2025-05-30T17:59:26.384725Z",
     "shell.execute_reply": "2025-05-30T17:59:26.384135Z",
     "shell.execute_reply.started": "2025-05-30T17:59:26.364562Z"
    },
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1703077857951,
     "user": {
      "displayName": "Martina Pticek",
      "userId": "05574592513673403901"
     },
     "user_tz": -60
    },
    "id": "HDXpGzg17kv6",
    "outputId": "958bf81c-6baf-4aaa-b51b-fbe6be9fdcb7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "from configparser import ConfigParser\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, main_conf_path):\n",
    "        self.main_conf_path = main_conf_path\n",
    "        self.main_config = self.read_config(os.path.join(main_conf_path, 'main_config.cfg'))\n",
    "\n",
    "    def read_config(self, conf_path):\n",
    "        conf_dict = OrderedDict()\n",
    "\n",
    "        config = ConfigParser()\n",
    "        config.read(conf_path)\n",
    "        for section in config.sections():\n",
    "            section_config = OrderedDict(config[section].items())\n",
    "            conf_dict[section] = self.type_ensurance(section_config)\n",
    "            self.__dict__.update((k, v) for k, v in conf_dict[section].items())\n",
    "\n",
    "        return conf_dict\n",
    "\n",
    "\n",
    "    def ensure_value_type(self, v):\n",
    "        BOOLEAN = {'false': False, 'False': False,\n",
    "                   'true': True, 'True': True}\n",
    "        if isinstance(v, str):\n",
    "            try:\n",
    "                value = eval(v)\n",
    "                if not isinstance(value, (str, int, float, list, tuple)):\n",
    "                    value = v\n",
    "            except:\n",
    "                if v in BOOLEAN:\n",
    "                    v = BOOLEAN[v]\n",
    "                value = v\n",
    "        else:\n",
    "            value = v\n",
    "        return value\n",
    "\n",
    "    def type_ensurance(self, config):\n",
    "        BOOLEAN = {'false': False, 'False': False,\n",
    "                   'true': True, 'True': True}\n",
    "\n",
    "        for k, v in config.items():\n",
    "            try:\n",
    "                value = eval(v)\n",
    "                if not isinstance(value, (str, int, float, list, tuple)):\n",
    "                    value = v\n",
    "            except:\n",
    "                if v in BOOLEAN:\n",
    "                    v = BOOLEAN[v]\n",
    "                value = v\n",
    "            config[k] = value\n",
    "        return config\n",
    "\n",
    "    def get_param(self, section, param):\n",
    "        if section in self.main_config:\n",
    "            section = self.main_config[section]\n",
    "        else:\n",
    "            raise NameError(\"There are not the parameter named '%s'\" % section)\n",
    "\n",
    "        if param in section:\n",
    "            value = section[param]\n",
    "        else:\n",
    "            raise NameError(\"There are not the parameter named '%s'\" % param)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def update_params(self, params):\n",
    "        # for now, assume 'params' is dictionary\n",
    "\n",
    "        for k, v in params.items():\n",
    "            updated=False\n",
    "            for section in self.main_config:\n",
    "                if k in self.main_config[section]:\n",
    "                    self.main_config[section][k] = self.ensure_value_type(v)\n",
    "                    self.__dict__[k] = self.main_config[section][k]\n",
    "                    updated = True\n",
    "\n",
    "                    break\n",
    "\n",
    "            if not updated:\n",
    "                # raise ValueError\n",
    "                print('Parameter not updated. \\'%s\\' not exists.' % k)\n",
    "\n",
    "\n",
    "    def save(self, base_dir):\n",
    "        def helper(section_k, section_v):\n",
    "            sec_str = '[%s]\\n' % section_k\n",
    "            for k, v in section_v.items():\n",
    "                sec_str += '%s=%s\\n' % (str(k), str(v))\n",
    "            sec_str += '\\n'\n",
    "            return sec_str\n",
    "\n",
    "        # save main config\n",
    "        main_conf_str =''\n",
    "        for section in self.main_config:\n",
    "            main_conf_str += helper(section, self.main_config[section])\n",
    "        with open(os.path.join(base_dir, 'main_config.cfg'), 'wt') as f:\n",
    "            f.write(main_conf_str)\n",
    "\n",
    "\n",
    "        print('main config saved in %s' % base_dir)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if not isinstance(item, str):\n",
    "            raise TypeError(\"index must be a str\")\n",
    "\n",
    "        if item in self.main_config:\n",
    "            section = self.main_config[item]\n",
    "        else:\n",
    "            raise NameError(\"There are not the parameter named '%s'\" % item)\n",
    "        return section\n",
    "\n",
    "    def __str__(self):\n",
    "        config_str = '\\n'\n",
    "\n",
    "        config_str += '>>>>> Main Config\\n'\n",
    "        for section in self.main_config:\n",
    "            config_str += '[%s]\\n' % section\n",
    "            config_str += '\\n'.join(['{}: {}'.format(k, self.main_config[section][k]) for k in self.main_config[section]])\n",
    "            config_str += '\\n\\n'\n",
    "\n",
    "        return config_str\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    param = Config('../main_config.cfg')\n",
    "\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tmOGSe57uQn"
   },
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T17:59:26.386442Z",
     "iopub.status.busy": "2025-05-30T17:59:26.386187Z",
     "iopub.status.idle": "2025-05-30T17:59:26.394687Z",
     "shell.execute_reply": "2025-05-30T17:59:26.394112Z",
     "shell.execute_reply.started": "2025-05-30T17:59:26.386417Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1703077858559,
     "user": {
      "displayName": "Martina Pticek",
      "userId": "05574592513673403901"
     },
     "user_tz": -60
    },
    "id": "f9LwLIEO7s4Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import strftime\n",
    "import logging\n",
    "\n",
    "\n",
    "def make_log_dir(log_dir):\n",
    "    \"\"\"\n",
    "    Generate directory path to log\n",
    "\n",
    "    :param log_dir:\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "\n",
    "    log_dirs = os.listdir(log_dir)\n",
    "    if len(log_dirs) == 0:\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx_list = sorted([int(d.split(\"_\")[0]) for d in log_dirs])\n",
    "        idx = idx_list[-1] + 1\n",
    "\n",
    "    cur_log_dir = \"%d_%s\" % (idx, strftime(\"%Y%m%d-%H%M\"))\n",
    "    full_log_dir = os.path.join(log_dir, cur_log_dir)\n",
    "    if not os.path.exists(full_log_dir):\n",
    "        os.mkdir(full_log_dir)\n",
    "\n",
    "    return full_log_dir\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, log_dir):\n",
    "        log_file_format = \"[%(lineno)d]%(asctime)s: %(message)s\"\n",
    "        log_console_format = \"%(message)s\"\n",
    "\n",
    "        # Main logger\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        self.logger = logging.getLogger(log_dir)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.propagate = False\n",
    "\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        console_handler.setFormatter(logging.Formatter(log_console_format))\n",
    "\n",
    "        file_handler = logging.FileHandler(os.path.join(log_dir, \"experiments.log\"))\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        file_handler.setFormatter(logging.Formatter(log_file_format))\n",
    "\n",
    "        self.logger.addHandler(console_handler)\n",
    "        self.logger.addHandler(file_handler)\n",
    "\n",
    "    def info(self, msg):\n",
    "        self.logger.info(msg)\n",
    "\n",
    "    def close(self):\n",
    "        for handle in self.logger.handlers[:]:\n",
    "            self.logger.removeHandler(handle)\n",
    "        logging.shutdown()\n",
    "\n",
    "\n",
    "def setup_logger(log_dir):\n",
    "    log_file_format = \"[%(lineno)d]%(asctime)s: %(message)s\"\n",
    "    log_console_format = \"%(message)s\"\n",
    "\n",
    "    # Main logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(logging.Formatter(log_console_format))\n",
    "\n",
    "    file_handler = logging.FileHandler(os.path.join(log_dir, \"experiments.log\"))\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(logging.Formatter(log_file_format))\n",
    "\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4C20SiZ7xYm"
   },
   "source": [
    "## Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T17:59:26.396130Z",
     "iopub.status.busy": "2025-05-30T17:59:26.395784Z",
     "iopub.status.idle": "2025-05-30T17:59:26.466916Z",
     "shell.execute_reply": "2025-05-30T17:59:26.466325Z",
     "shell.execute_reply.started": "2025-05-30T17:59:26.396098Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1703077858559,
     "user": {
      "displayName": "Martina Pticek",
      "userId": "05574592513673403901"
     },
     "user_tz": -60
    },
    "id": "znEUw9H671H3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ResultTable:\n",
    "    \"\"\"\n",
    "\n",
    "    Class to save and show result neatly.\n",
    "    First column is always 'NAME' column.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, table_name='table', header=None, splitter='||', int_formatter='%3d', float_formatter='%.4f'):\n",
    "        \"\"\"\n",
    "        Initialize table setting.\n",
    "\n",
    "        :param list header: list of string, table headers.\n",
    "        :param str splitter:\n",
    "        :param str int_formatter:\n",
    "        :param str float_formatter:\n",
    "        \"\"\"\n",
    "        self.table_name = table_name\n",
    "        self.header = header\n",
    "        if self.header is not None:\n",
    "            self.set_headers(self.header)\n",
    "        self.num_rows = 0\n",
    "        self.splitter = splitter\n",
    "        self.int_formatter = int_formatter\n",
    "        self.float_formatter = float_formatter\n",
    "\n",
    "    def set_headers(self, header):\n",
    "        \"\"\"\n",
    "        Set table headers as given and clear all data.\n",
    "\n",
    "        :param list header: list of header strings\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.header = header\n",
    "        if 'NAME' not in header:\n",
    "            self.header = ['NAME'] + self.header\n",
    "        self.data = OrderedDict([(h, []) for h in self.header])\n",
    "        self.max_len = OrderedDict([(h, len(h)) for h in self.header])\n",
    "        # {h: len(h) for h in self.header}\n",
    "\n",
    "    def add_row(self, row_name, row_dict):\n",
    "        \"\"\"\n",
    "        Add new row into the table.\n",
    "\n",
    "        :param str row_name: name of the row, which will be the first column\n",
    "        :param dict row_dict: dictionary containing column name as a key and column value as value.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # If header is not defined, fetch from input dict\n",
    "        if self.header is None:\n",
    "            self.set_headers(list(row_dict.keys()))\n",
    "\n",
    "        # If input dict has new column, make one\n",
    "        for key in row_dict:\n",
    "            if key not in self.data:\n",
    "                self.set_headers(self.header + [key])\n",
    "\n",
    "        for h in self.header:\n",
    "            if h == 'NAME':\n",
    "                self.data['NAME'].append(row_name)\n",
    "                self.max_len[h] = max(self.max_len['NAME'], len(row_name))\n",
    "            else:\n",
    "                # If input dict doesn't have values for table header, make empty value.\n",
    "                if h not in row_dict:\n",
    "                    row_dict[h] = '-'\n",
    "\n",
    "                # convert input dict to string\n",
    "                d = row_dict[h]\n",
    "\n",
    "                if isinstance(d, (int, np.integer)):\n",
    "                    d_str = self.int_formatter % d\n",
    "                elif isinstance(d, (float, np.float)):\n",
    "                    d_str = self.float_formatter % d\n",
    "                elif isinstance(d, str):\n",
    "                    d_str = d\n",
    "                elif isinstance(d, list):\n",
    "                    d_str = str(d)\n",
    "                else:\n",
    "                    raise NotImplementedError('data type currently not supported. %s' % str(type(d)))\n",
    "\n",
    "                self.data[h].append(d_str)\n",
    "                self.max_len[h] = max(self.max_len[h], len(d_str))\n",
    "        self.num_rows += 1\n",
    "\n",
    "    def row_to_line(self, row_values):\n",
    "        \"\"\"\n",
    "        Convert a row into string form\n",
    "\n",
    "        :param list row_values: list of row values as string\n",
    "        :return: string form of a row\n",
    "        \"\"\"\n",
    "        value_str = []\n",
    "        for i, header in enumerate(self.header):\n",
    "            max_length = self.max_len[header]\n",
    "            length = len(row_values[i])\n",
    "            diff = max_length - length\n",
    "\n",
    "            # Center align\n",
    "            # left_space = diff // 2\n",
    "            # right_space = diff - left_space\n",
    "            # s = ' ' * left_space + row_values[i] + ' ' * right_space\n",
    "\n",
    "            # Left align\n",
    "            s = row_values[i] + ' ' * diff\n",
    "            value_str.append(s)\n",
    "\n",
    "        # for i, max_length in enumerate(self.max_len.values()):\n",
    "        #     length = len(row_values[i])\n",
    "        #     diff = max_length - length\n",
    "        #\n",
    "        #     # Center align\n",
    "        #     # left_space = diff // 2\n",
    "        #     # right_space = diff - left_space\n",
    "        #     # s = ' ' * left_space + row_values[i] + ' ' * right_space\n",
    "        #\n",
    "        #     # Left align\n",
    "        #     s = row_values[i] + ' ' * diff\n",
    "        #     value_str.append(s)\n",
    "\n",
    "        return self.splitter + ' ' + (' %s ' % self.splitter).join(value_str) + ' ' + self.splitter\n",
    "\n",
    "    def to_string(self):\n",
    "        \"\"\"\n",
    "        Convert a table into string form\n",
    "\n",
    "        :return: string form of the table\n",
    "        \"\"\"\n",
    "        size_per_col = {h: self.max_len[h] + 2 + len(self.splitter) for h in self.header}\n",
    "        line_len = sum([size_per_col[c] for c in size_per_col]) + len(self.splitter)\n",
    "        table_str = '\\n'\n",
    "\n",
    "        # TABLE NAME\n",
    "        table_str += self.table_name + '\\n'\n",
    "\n",
    "        # HEADER\n",
    "        line = self.row_to_line(self.header)\n",
    "        table_str += '=' * line_len + '\\n'\n",
    "        table_str += line + '\\n'\n",
    "        table_str += self.splitter + '-' * (line_len - len(self.splitter) * 2) + self.splitter + '\\n'\n",
    "\n",
    "        # DATA\n",
    "        for row_values in zip(*self.data.values()):\n",
    "            line = self.row_to_line(row_values)\n",
    "            table_str += line + '\\n'\n",
    "        table_str += '=' * line_len + '\\n'\n",
    "        return table_str\n",
    "\n",
    "    def show(self):\n",
    "        print(self.to_string())\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return (self.num_rows, self.num_cols)\n",
    "\n",
    "    @property\n",
    "    def num_cols(self):\n",
    "        return len(self.header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3mkslbA89Uo"
   },
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T17:59:26.467880Z",
     "iopub.status.busy": "2025-05-30T17:59:26.467649Z",
     "iopub.status.idle": "2025-05-30T17:59:26.474408Z",
     "shell.execute_reply": "2025-05-30T17:59:26.473900Z",
     "shell.execute_reply.started": "2025-05-30T17:59:26.467862Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1703077858559,
     "user": {
      "displayName": "Martina Pticek",
      "userId": "05574592513673403901"
     },
     "user_tz": -60
    },
    "id": "Vjnxoozh8IWk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Statistics:\n",
    "    def __init__(self, name='AVG'):\n",
    "        self.name = name\n",
    "        self.history = []\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.history.append(val)\n",
    "        self.sum += val\n",
    "        self.cnt += 1\n",
    "\n",
    "    @property\n",
    "    def mean_std(self):\n",
    "        # mean = self.sum / self.cnt\n",
    "        mean = np.mean(self.history)\n",
    "        std = np.std(self.history)\n",
    "        return mean, std\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        # return self.sum / self.cnt\n",
    "        return np.mean(self.history)\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.std(self.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFAzLCVg9BLe"
   },
   "source": [
    "## Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T17:59:26.475511Z",
     "iopub.status.busy": "2025-05-30T17:59:26.475364Z",
     "iopub.status.idle": "2025-05-30T17:59:27.824368Z",
     "shell.execute_reply": "2025-05-30T17:59:27.823703Z",
     "shell.execute_reply.started": "2025-05-30T17:59:26.475497Z"
    },
    "executionInfo": {
     "elapsed": 1862,
     "status": "ok",
     "timestamp": 1703077860419,
     "user": {
      "displayName": "Martina Pticek",
      "userId": "05574592513673403901"
     },
     "user_tz": -60
    },
    "id": "ZHyv3MqB8LHy"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def getlocaltime():\n",
    "    date = time.strftime('%y-%m-%d', time.localtime())\n",
    "    current_time = time.strftime('%H:%M:%S', time.localtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJBfoAYp8PNJ"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6XLad258cDf"
   },
   "source": [
    "## Run Classifier dataset utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T17:59:27.826704Z",
     "iopub.status.busy": "2025-05-30T17:59:27.826367Z",
     "iopub.status.idle": "2025-05-30T17:59:29.624442Z",
     "shell.execute_reply": "2025-05-30T17:59:29.623478Z",
     "shell.execute_reply.started": "2025-05-30T17:59:27.826703Z"
    },
    "executionInfo": {
     "elapsed": 741,
     "status": "ok",
     "timestamp": 1703077861158,
     "user": {
      "displayName": "Martina Pticek",
      "userId": "05574592513673403901"
     },
     "user_tz": -60
    },
    "id": "SKm8o5iz60uk"
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" BERT classification fine-tuning: utilities to work with GLUE tasks \"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, truncnorm\n",
    "from sklearn.metrics import (\n",
    "    matthews_corrcoef,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    mean_squared_error,\n",
    "    classification_report,\n",
    ")\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        guid,\n",
    "        text_a,\n",
    "        text_b=None,\n",
    "        label=None,\n",
    "        POS=None,\n",
    "        FGPOS=None,\n",
    "        text_a_2=None,\n",
    "        text_b_2=None,\n",
    "    ):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        self.POS = POS\n",
    "        self.FGPOS = FGPOS\n",
    "        self.text_a_2 = text_a_2\n",
    "        self.text_b_2 = text_b_2\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_ids,\n",
    "        input_mask,\n",
    "        segment_ids,\n",
    "        label_id,\n",
    "        guid=None,\n",
    "        input_ids_2=None,\n",
    "        input_mask_2=None,\n",
    "        segment_ids_2=None,\n",
    "    ):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.guid = guid\n",
    "        self.input_ids_2 = input_ids_2\n",
    "        self.input_mask_2 = input_mask_2\n",
    "        self.segment_ids_2 = segment_ids_2\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, \"utf-8\") for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "\n",
    "class TrofiProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the TroFi and MOH-X data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir, k=None):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        if k is not None:\n",
    "            return self._create_examples(\n",
    "                self._read_tsv(os.path.join(data_dir, \"train\" + str(k) + \".tsv\")), \"train\"\n",
    "            )\n",
    "        else:\n",
    "            return self._create_examples(\n",
    "                self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\"\n",
    "            )\n",
    "\n",
    "    def get_test_examples(self, data_dir, k=None):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        if k is not None:\n",
    "            return self._create_examples(\n",
    "                self._read_tsv(os.path.join(data_dir, \"test\" + str(k) + \".tsv\")), \"test\"\n",
    "            )\n",
    "        else:\n",
    "            return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir, k=None):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        if k is not None:\n",
    "            return self._create_examples(\n",
    "                self._read_tsv(os.path.join(data_dir, \"dev\" + str(k) + \".tsv\")), \"dev\"\n",
    "            )\n",
    "        else:\n",
    "            return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[2]\n",
    "            label = line[1]\n",
    "            POS = line[3]\n",
    "            FGPOS = line[4]\n",
    "            index = line[-1]\n",
    "            examples.append(\n",
    "                InputExample(\n",
    "                    guid=guid, text_a=text_a, text_b=index, label=label, POS=POS, FGPOS=FGPOS\n",
    "                )\n",
    "            )\n",
    "        return examples\n",
    "\n",
    "\n",
    "class VUAProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the VUA data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[2]\n",
    "            label = line[1]\n",
    "            POS = line[3]\n",
    "            FGPOS = line[4]\n",
    "            if len(line) == 8:\n",
    "                index = line[5]\n",
    "                text_a_2 = line[6]\n",
    "                index_2 = line[7]\n",
    "                examples.append(\n",
    "                    InputExample(\n",
    "                        guid=guid,\n",
    "                        text_a=text_a,\n",
    "                        text_b=index,\n",
    "                        label=label,\n",
    "                        POS=POS,\n",
    "                        FGPOS=FGPOS,\n",
    "                        text_a_2=text_a_2,\n",
    "                        text_b_2=index_2,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                index = line[-1]\n",
    "                examples.append(\n",
    "                    InputExample(\n",
    "                        guid=guid, text_a=text_a, text_b=index, label=label, POS=POS, FGPOS=FGPOS\n",
    "                    )\n",
    "                )\n",
    "        return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(\n",
    "    examples, label_list, max_seq_length, tokenizer, output_mode, args\n",
    "):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)  # tokenize the sentence\n",
    "        tokens_b = None\n",
    "\n",
    "        try:\n",
    "            text_b = int(example.text_b)  # index of target word\n",
    "            tokens_b = text_b\n",
    "\n",
    "            # truncate the sentence to max_seq_len\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[: (max_seq_length - 2)]\n",
    "\n",
    "            # Find the target word index\n",
    "            for i, w in enumerate(example.text_a.split()):\n",
    "                # If w is a target word, tokenize the word and save to text_b\n",
    "                if i == text_b:\n",
    "                    # consider the index due to models that use a byte-level BPE as a tokenizer (e.g., GPT2, RoBERTa)\n",
    "                    text_b = tokenizer.tokenize(w) if i == 0 else tokenizer.tokenize(\" \" + w)\n",
    "                    break\n",
    "                w_tok = tokenizer.tokenize(w) if i == 0 else tokenizer.tokenize(\" \" + w)\n",
    "\n",
    "                # Count number of tokens before the target word to get the target word index\n",
    "                if w_tok:\n",
    "                    tokens_b += len(w_tok) - 1\n",
    "\n",
    "        except TypeError:\n",
    "            if example.text_b:\n",
    "                tokens_b = tokenizer.tokenize(example.text_b)\n",
    "                # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "                # length is less than the specified length.\n",
    "                # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "                _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "            else:\n",
    "                # Account for [CLS] and [SEP] with \"- 2\"\n",
    "                if len(tokens_a) > max_seq_length - 2:\n",
    "                    tokens_a = tokens_a[: (max_seq_length - 2)]\n",
    "\n",
    "        tokens = [tokenizer.cls_token] + tokens_a + [tokenizer.sep_token]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # set the target word as 1 in segment ids\n",
    "        try:\n",
    "            tokens_b += 1  # add 1 to the target word index considering [CLS]\n",
    "            for i in range(len(text_b)):\n",
    "                segment_ids[tokens_b + i] = 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [tokenizer.convert_tokens_to_ids(tokenizer.pad_token)] * (\n",
    "            max_seq_length - len(input_ids)\n",
    "        )\n",
    "        input_ids += padding\n",
    "        input_mask += [0] * len(padding)\n",
    "        segment_ids += [0] * len(padding)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if output_mode == \"classification\":\n",
    "            label_id = label_map[example.label]\n",
    "        else:\n",
    "            raise KeyError(output_mode)\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %s)\" % (example.label, str(label_id)))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                label_id=label_id,\n",
    "                guid=example.guid + \" \" + str(example.text_b),\n",
    "            )\n",
    "        )\n",
    "    return features\n",
    "\n",
    "\n",
    "def convert_two_examples_to_features(\n",
    "    examples, label_list, max_seq_length, tokenizer, output_mode, win_size=-1\n",
    "):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)  # tokenize the sentence\n",
    "        tokens_b = None\n",
    "        text_b = None\n",
    "\n",
    "        try:\n",
    "            text_b = int(example.text_b)  # index of target word\n",
    "            tokens_b = text_b\n",
    "\n",
    "            # truncate the sentence to max_seq_len\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[: (max_seq_length - 2)]\n",
    "\n",
    "            # Find the target word index\n",
    "            for i, w in enumerate(example.text_a.split()):\n",
    "                # If w is a target word, tokenize the word and save to text_b\n",
    "                if i == text_b:\n",
    "                    # consider the index due to models that use a byte-level BPE as a tokenizer (e.g., GPT2, RoBERTa)\n",
    "                    text_b = tokenizer.tokenize(w) if i == 0 else tokenizer.tokenize(\" \" + w)\n",
    "                    break\n",
    "                w_tok = tokenizer.tokenize(w) if i == 0 else tokenizer.tokenize(\" \" + w)\n",
    "\n",
    "                # Count number of tokens before the target word to get the target word index\n",
    "                if w_tok:\n",
    "                    tokens_b += len(w_tok) - 1\n",
    "\n",
    "        except TypeError:\n",
    "            if example.text_b:\n",
    "                tokens_b = tokenizer.tokenize(example.text_b)\n",
    "                # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "                # length is less than the specified length.\n",
    "                # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "                _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "            else:\n",
    "                # Account for [CLS] and [SEP] with \"- 2\"\n",
    "                if len(tokens_a) > max_seq_length - 2:\n",
    "                    tokens_a = tokens_a[: (max_seq_length - 2)]\n",
    "\n",
    "        tokens = [tokenizer.cls_token] + tokens_a + [tokenizer.sep_token]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        # set the target word as 1 in segment ids\n",
    "        try:\n",
    "            tokens_b += 1  # add 1 to the target word index considering [CLS]\n",
    "            for i in range(len(text_b)):\n",
    "                segment_ids[tokens_b + i] = 1\n",
    "\n",
    "            # concatentate the second sentence ( [\"[CLS]\"] + tokens_a + [\"[SEP]\"] -> [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + text_b + [\"[SEP]\"])\n",
    "            tokens = tokens + text_b + [tokenizer.sep_token]\n",
    "            segment_ids = segment_ids + [0] * len(text_b)\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [tokenizer.convert_tokens_to_ids(tokenizer.pad_token)] * (\n",
    "            max_seq_length - len(input_ids)\n",
    "        )\n",
    "        input_ids += padding\n",
    "        input_mask += [0] * len(padding)\n",
    "        segment_ids += [0] * len(padding)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if output_mode == \"classification\":\n",
    "            label_id = label_map[example.label]\n",
    "        else:\n",
    "            raise KeyError(output_mode)\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %s)\" % (example.label, str(label_id)))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                label_id=label_id,\n",
    "                guid=example.guid + \" \" + example.text_b,\n",
    "            )\n",
    "        )\n",
    "    return features\n",
    "\n",
    "\n",
    "def convert_examples_to_two_features(\n",
    "    examples, label_list, max_seq_length, tokenizer, output_mode, args\n",
    "):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)  # tokenize the sentence\n",
    "        tokens_b = None\n",
    "        text_b = None\n",
    "\n",
    "        try:\n",
    "            text_b = int(example.text_b)  # index of target word\n",
    "            tokens_b = text_b\n",
    "\n",
    "            # truncate the sentence to max_seq_len\n",
    "            if len(tokens_a) > max_seq_length - 6:\n",
    "                tokens_a = tokens_a[: (max_seq_length - 6)]\n",
    "\n",
    "            # Find the target word index\n",
    "            for i, w in enumerate(example.text_a.split()):\n",
    "                # If w is a target word, tokenize the word and save to text_b\n",
    "                if i == text_b:\n",
    "                    # consider the index due to models that use a byte-level BPE as a tokenizer (e.g., GPT2, RoBERTa)\n",
    "                    text_b = tokenizer.tokenize(w) if i == 0 else tokenizer.tokenize(\" \" + w)\n",
    "                    break\n",
    "\n",
    "                w_tok = tokenizer.tokenize(w) if i == 0 else tokenizer.tokenize(\" \" + w)\n",
    "\n",
    "                # Count number of tokens before the target word to get the target word index\n",
    "                if w_tok:\n",
    "                    tokens_b += len(w_tok) - 1\n",
    "\n",
    "            if tokens_b + len(text_b) > max_seq_length - 6:\n",
    "                continue\n",
    "\n",
    "        except TypeError:\n",
    "            if example.text_b:\n",
    "                tokens_b = tokenizer.tokenize(example.text_b)\n",
    "                # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "                _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "            else:\n",
    "                # Account for [CLS] and [SEP] with \"- 2\"\n",
    "                if len(tokens_a) > max_seq_length - 2:\n",
    "                    tokens_a = tokens_a[: (max_seq_length - 2)]\n",
    "\n",
    "        tokens = [tokenizer.cls_token] + tokens_a + [tokenizer.sep_token]\n",
    "\n",
    "        # POS tag tokens\n",
    "        if args.use_pos:\n",
    "            POS_token = tokenizer.tokenize(example.POS)\n",
    "            tokens += POS_token + [tokenizer.sep_token]\n",
    "\n",
    "        # Local context\n",
    "        if args.use_local_context:\n",
    "            local_start = 1\n",
    "            local_end = local_start + len(tokens_a)\n",
    "            comma1 = tokenizer.tokenize(\",\")[0]\n",
    "            comma2 = tokenizer.tokenize(\" ,\")[0]\n",
    "            for i, w in enumerate(tokens):\n",
    "                if i < tokens_b + 1 and (w in [comma1, comma2]):\n",
    "                    local_start = i\n",
    "                if i > tokens_b + 1 and (w in [comma1, comma2]):\n",
    "                    local_end = i\n",
    "                    break\n",
    "            segment_ids = [\n",
    "                2 if i >= local_start and i <= local_end else 0 for i in range(len(tokens))\n",
    "            ]\n",
    "        else:\n",
    "            segment_ids = [0] * len(tokens)\n",
    "\n",
    "        # POS tag encoding\n",
    "        after_token_a = False\n",
    "        for i, t in enumerate(tokens):\n",
    "            if t == tokenizer.sep_token:\n",
    "                after_token_a = True\n",
    "            if after_token_a and t != tokenizer.sep_token:\n",
    "                segment_ids[i] = 3\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        try:\n",
    "            tokens_b += 1  # add 1 to the target word index considering [CLS]\n",
    "            for i in range(len(text_b)):\n",
    "                segment_ids[tokens_b + i] = 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        padding = [tokenizer.convert_tokens_to_ids(tokenizer.pad_token)] * (\n",
    "            max_seq_length - len(input_ids)\n",
    "        )\n",
    "        input_ids += padding\n",
    "        input_mask += [0] * len(padding)\n",
    "        segment_ids += [0] * len(padding)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if output_mode == \"classification\":\n",
    "            label_id = label_map[example.label]\n",
    "        else:\n",
    "            raise KeyError(output_mode)\n",
    "\n",
    "        # Second features (Target word)\n",
    "        tokens = [tokenizer.cls_token] + text_b + [tokenizer.sep_token]\n",
    "        segment_ids_2 = [0] * len(tokens)\n",
    "        try:\n",
    "            tokens_b = 1  # add 1 to the target word index considering [CLS]\n",
    "            for i in range(len(text_b)):\n",
    "                segment_ids_2[tokens_b + i] = 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        input_ids_2 = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask_2 = [1] * len(input_ids_2)\n",
    "\n",
    "        padding = [tokenizer.convert_tokens_to_ids(tokenizer.pad_token)] * (\n",
    "            max_seq_length - len(input_ids_2)\n",
    "        )\n",
    "        input_ids_2 += padding\n",
    "        input_mask_2 += [0] * len(padding)\n",
    "        segment_ids_2 += [0] * len(padding)\n",
    "\n",
    "        assert len(input_ids_2) == max_seq_length\n",
    "        assert len(input_mask_2) == max_seq_length\n",
    "        assert len(segment_ids_2) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                label_id=label_id,\n",
    "                guid=example.guid + \" \" + str(example.text_b),\n",
    "                input_ids_2=input_ids_2,\n",
    "                input_mask_2=input_mask_2,\n",
    "                segment_ids_2=segment_ids_2,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def seq_accuracy(preds, labels):\n",
    "    acc = []\n",
    "    for idx, pred in enumerate(preds):\n",
    "        acc.append((pred == labels[idx]).mean())\n",
    "    return acc.mean()\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'simple_accuracy' is equivalent to sklearn's accuracy_score\n",
    "def simple_accuracy(preds, labels):\n",
    "    return accuracy_score(labels, preds)\n",
    "\n",
    "def acc_and_f1(preds_probs, labels):\n",
    "    \"\"\"\n",
    "    Calculates accuracy and F1 score.\n",
    "    preds_probs: Probability scores for the positive class.\n",
    "    labels: True binary labels.\n",
    "    \"\"\"\n",
    "    # Convert probabilities to binary predictions using a 0.5 threshold\n",
    "    preds_binary = np.array(preds_probs) >= 0.5\n",
    "    acc = simple_accuracy(preds_binary, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds_binary)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"acc_and_f1\": (acc + f1) / 2,\n",
    "    }\n",
    "\n",
    "def all_metrics(preds_probs, labels, plot_roc_curve=True):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, precision, recall, F1-score, classification report,\n",
    "    ROC AUC score, and optionally plots the ROC curve.\n",
    "\n",
    "    Args:\n",
    "        preds_probs (array-like): Probability scores for the positive class.\n",
    "        labels (array-like): True binary labels (0 or 1).\n",
    "        plot_roc_curve (bool): Whether to plot the ROC curve.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    # Ensure labels are 0 or 1 for roc_auc_score and roc_curve\n",
    "    if not all(label in [0, 1] for label in np.unique(labels)):\n",
    "        raise ValueError(\"Labels for ROC calculation must be binary (0 or 1).\")\n",
    "\n",
    "    # Convert probabilities to binary predictions using a 0.5 threshold\n",
    "    # for standard classification metrics\n",
    "    preds_binary = (np.array(preds_probs) >= 0.5).astype(int)\n",
    "\n",
    "    acc = simple_accuracy(preds_binary, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds_binary, zero_division=0)\n",
    "    pre = precision_score(y_true=labels, y_pred=preds_binary, zero_division=0)\n",
    "    rec = recall_score(y_true=labels, y_pred=preds_binary, zero_division=0)\n",
    "    \n",
    "    print(\"Classification Report (based on 0.5 threshold):\")\n",
    "    try:\n",
    "        clas_report = classification_report(y_true=labels, y_pred=preds_binary, digits=6, zero_division=0)\n",
    "        print(clas_report)\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not generate classification report: {e}\")\n",
    "        clas_report = \"N/A\"\n",
    "\n",
    "    # ROC AUC Score\n",
    "    # Ensure there are both classes present in labels for roc_auc_score\n",
    "    roc_auc = None\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(labels, preds_probs)\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not calculate ROC AUC: {e}. Ensure labels are binary and scores are continuous.\")\n",
    "            roc_auc = 0.0 # Or handle as appropriate\n",
    "    else:\n",
    "        print(\"ROC AUC not computed because only one class is present in labels.\")\n",
    "        roc_auc = 0.0\n",
    "\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"acc\": acc,\n",
    "        \"precision\": pre,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc\n",
    "    }\n",
    "\n",
    "    if plot_roc_curve and roc_auc is not None and len(np.unique(labels)) > 1:\n",
    "        fpr, tpr, thresholds = roc_curve(labels, preds_probs)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC krivulja (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Stopa lano pozitivnih rezultata')\n",
    "        plt.ylabel('Stopa tono pozitivnih rezultata')\n",
    "        plt.title('Krivulja ROC - stopa uenja 2e-5')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        # If in a Jupyter Notebook, plt.show() is usually sufficient.\n",
    "        # To save the plot:\n",
    "        #plt.savefig('roc_curve_xlm-r-bertic-3e5-4.png')\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def compute_metrics(preds_probs, labels):\n",
    "    \"\"\"\n",
    "    Wrapper function that calls all_metrics.\n",
    "    Assumes preds_probs are the probability scores for the positive class.\n",
    "    \"\"\"\n",
    "    assert len(preds_probs) == len(labels), \"Predictions and labels must have the same length.\"\n",
    "    return all_metrics(preds_probs, labels)\n",
    "\n",
    "\n",
    "processors = {\n",
    "    \"vua\": VUAProcessor,\n",
    "    \"trofi\": TrofiProcessor,\n",
    "}\n",
    "\n",
    "output_modes = {\n",
    "    \"vua\": \"classification\",\n",
    "    \"trofi\": \"classification\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me13nGEg7S9V"
   },
   "source": [
    "## Data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T17:59:29.625891Z",
     "iopub.status.busy": "2025-05-30T17:59:29.625614Z",
     "iopub.status.idle": "2025-05-30T17:59:29.647552Z",
     "shell.execute_reply": "2025-05-30T17:59:29.644307Z",
     "shell.execute_reply.started": "2025-05-30T17:59:29.625868Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1703077861158,
     "user": {
      "displayName": "Martina Pticek",
      "userId": "05574592513673403901"
     },
     "user_tz": -60
    },
    "id": "iDaoZyeO7Wg5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "'''\n",
    "from run_classifier_dataset_utils import (\n",
    "    convert_examples_to_two_features,\n",
    "    convert_examples_to_features,\n",
    "    convert_two_examples_to_features,\n",
    ")\n",
    "'''\n",
    "\n",
    "def load_train_data(args, logger, processor, task_name, label_list, tokenizer, output_mode, k=None):\n",
    "    # Prepare data loader\n",
    "    if task_name == \"vua\":\n",
    "        train_examples = processor.get_train_examples(args.data_dir)\n",
    "    elif task_name == \"trofi\":\n",
    "        train_examples = processor.get_train_examples(args.data_dir, k)\n",
    "    else:\n",
    "        raise (\"task_name should be 'vua' or 'trofi'!\")\n",
    "\n",
    "    # make features file\n",
    "    if args.model_type == \"BERT_BASE\":\n",
    "        train_features = convert_two_examples_to_features(\n",
    "            train_examples, label_list, args.max_seq_length, tokenizer, output_mode\n",
    "        )\n",
    "    if args.model_type in [\"BERT_SEQ\", \"MELBERT_SPV\"]:\n",
    "        train_features = convert_examples_to_features(\n",
    "            train_examples, label_list, args.max_seq_length, tokenizer, output_mode, args\n",
    "        )\n",
    "    if args.model_type in [\"MELBERT_MIP\", \"MELBERT\"]:\n",
    "        train_features = convert_examples_to_two_features(\n",
    "            train_examples, label_list, args.max_seq_length, tokenizer, output_mode, args\n",
    "        )\n",
    "\n",
    "    # make features into tensor\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "\n",
    "    # add additional features for MELBERT_MIP and MELBERT\n",
    "    if args.model_type in [\"MELBERT_MIP\", \"MELBERT\"]:\n",
    "        all_input_ids_2 = torch.tensor([f.input_ids_2 for f in train_features], dtype=torch.long)\n",
    "        all_input_mask_2 = torch.tensor([f.input_mask_2 for f in train_features], dtype=torch.long)\n",
    "        all_segment_ids_2 = torch.tensor(\n",
    "            [f.segment_ids_2 for f in train_features], dtype=torch.long\n",
    "        )\n",
    "        train_data = TensorDataset(\n",
    "            all_input_ids,\n",
    "            all_input_mask,\n",
    "            all_segment_ids,\n",
    "            all_label_ids,\n",
    "            all_input_ids_2,\n",
    "            all_input_mask_2,\n",
    "            all_segment_ids_2,\n",
    "        )\n",
    "    else:\n",
    "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, sampler=train_sampler, batch_size=args.train_batch_size\n",
    "    )\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def load_train_data_kf(\n",
    "    args, logger, processor, task_name, label_list, tokenizer, output_mode, k=None\n",
    "):\n",
    "    # Prepare data loader\n",
    "    if task_name == \"vua\":\n",
    "        train_examples = processor.get_train_examples(args.data_dir)\n",
    "    elif task_name == \"trofi\":\n",
    "        train_examples = processor.get_train_examples(args.data_dir, k)\n",
    "    else:\n",
    "        raise (\"task_name should be 'vua' or 'trofi'!\")\n",
    "\n",
    "    # make features file\n",
    "    if args.model_type == \"BERT_BASE\":\n",
    "        train_features = convert_two_examples_to_features(\n",
    "            train_examples, label_list, args.max_seq_length, tokenizer, output_mode\n",
    "        )\n",
    "    if args.model_type in [\"BERT_SEQ\", \"MELBERT_SPV\"]:\n",
    "        train_features = convert_examples_to_features(\n",
    "            train_examples, label_list, args.max_seq_length, tokenizer, output_mode, args\n",
    "        )\n",
    "    if args.model_type in [\"MELBERT_MIP\", \"MELBERT\"]:\n",
    "        train_features = convert_examples_to_two_features(\n",
    "            train_examples, label_list, args.max_seq_length, tokenizer, output_mode, args\n",
    "        )\n",
    "\n",
    "    # make features into tensor\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "\n",
    "    # add additional features for MELBERT_MIP and MELBERT\n",
    "    if args.model_type in [\"MELBERT_MIP\", \"MELBERT\"]:\n",
    "        all_input_ids_2 = torch.tensor([f.input_ids_2 for f in train_features], dtype=torch.long)\n",
    "        all_input_mask_2 = torch.tensor([f.input_mask_2 for f in train_features], dtype=torch.long)\n",
    "        all_segment_ids_2 = torch.tensor(\n",
    "            [f.segment_ids_2 for f in train_features], dtype=torch.long\n",
    "        )\n",
    "        train_data = TensorDataset(\n",
    "            all_input_ids,\n",
    "            all_input_mask,\n",
    "            all_segment_ids,\n",
    "            all_label_ids,\n",
    "            all_input_ids_2,\n",
    "            all_input_mask_2,\n",
    "            all_segment_ids_2,\n",
    "        )\n",
    "    else:\n",
    "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    gkf = StratifiedKFold(n_splits=args.num_bagging).split(X=all_input_ids, y=all_label_ids.numpy())\n",
    "    return train_data, gkf\n",
    "\n",
    "\n",
    "def load_test_data(args, logger, processor, task_name, label_list, tokenizer, output_mode, k=None):\n",
    "    if task_name == \"vua\":\n",
    "        eval_examples = processor.get_test_examples(args.data_dir)\n",
    "    elif task_name == \"trofi\":\n",
    "        eval_examples = processor.get_test_examples(args.data_dir, k)\n",
    "    else:\n",
    "        raise (\"task_name should be 'vua' or 'trofi'!\")\n",
    "\n",
    "    if args.model_type == \"BERT_BASE\":\n",
    "        eval_features = convert_two_examples_to_features(\n",
    "            eval_examples, label_list, args.max_seq_length, tokenizer, output_mode\n",
    "        )\n",
    "    if args.model_type in [\"BERT_SEQ\", \"MELBERT_SPV\"]:\n",
    "        eval_features = convert_examples_to_features(\n",
    "            eval_examples, label_list, args.max_seq_length, tokenizer, output_mode, args\n",
    "        )\n",
    "    if args.model_type in [\"MELBERT_MIP\", \"MELBERT\"]:\n",
    "        eval_features = convert_examples_to_two_features(\n",
    "            eval_examples, label_list, args.max_seq_length, tokenizer, output_mode, args\n",
    "        )\n",
    "\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    if args.model_type in [\"MELBERT_MIP\", \"MELBERT\"]:\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_guids = [f.guid for f in eval_features]\n",
    "        all_idx = torch.tensor([i for i in range(len(eval_features))], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "        all_input_ids_2 = torch.tensor([f.input_ids_2 for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask_2 = torch.tensor([f.input_mask_2 for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids_2 = torch.tensor([f.segment_ids_2 for f in eval_features], dtype=torch.long)\n",
    "        eval_data = TensorDataset(\n",
    "            all_input_ids,\n",
    "            all_input_mask,\n",
    "            all_segment_ids,\n",
    "            all_label_ids,\n",
    "            all_idx,\n",
    "            all_input_ids_2,\n",
    "            all_input_mask_2,\n",
    "            all_segment_ids_2,\n",
    "        )\n",
    "    else:\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_guids = [f.guid for f in eval_features]\n",
    "        all_idx = torch.tensor([i for i in range(len(eval_features))], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "        eval_data = TensorDataset(\n",
    "            all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_idx\n",
    "        )\n",
    "\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    return all_guids, eval_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWqazG8X9UY5"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T17:59:29.649082Z",
     "iopub.status.busy": "2025-05-30T17:59:29.648901Z",
     "iopub.status.idle": "2025-05-30T17:59:30.009601Z",
     "shell.execute_reply": "2025-05-30T17:59:30.008997Z",
     "shell.execute_reply.started": "2025-05-30T17:59:29.649065Z"
    },
    "executionInfo": {
     "elapsed": 1066,
     "status": "ok",
     "timestamp": 1703077862221,
     "user": {
      "displayName": "Martina Pticek",
      "userId": "05574592513673403901"
     },
     "user_tz": -60
    },
    "id": "uV12TBtE9Xkx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from utils import Config\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "class AutoModelForSequenceClassification(nn.Module):\n",
    "    \"\"\"Base model for sequence classification\"\"\"\n",
    "\n",
    "    def __init__(self, args, Model, config, num_labels=2):\n",
    "        \"\"\"Initialize the model\"\"\"\n",
    "        super(AutoModelForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.encoder = Model\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(args.drop_ratio)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self._init_weights(self.classifier)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        target_mask=None,\n",
    "        token_type_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        head_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary\n",
    "            `target_mask`: a torch.LongTensor of shape [batch_size, sequence_length] with the mask for target wor. 1 for target word and 0 otherwise.\n",
    "            `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices\n",
    "                selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n",
    "            `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1].\n",
    "                It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch.\n",
    "                It's the mask that we typically use for attention when a batch has varying length sentences.\n",
    "            `labels`: optional labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n",
    "                with indices selected in [0, ..., num_labels].\n",
    "            `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
    "                It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
    "        \"\"\"\n",
    "        outputs = self.encoder(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        logits = self.logsoftmax(logits)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.NLLLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        return logits\n",
    "\n",
    "\n",
    "class AutoModelForTokenClassification(nn.Module):\n",
    "    \"\"\"Base model for token classification\"\"\"\n",
    "\n",
    "    def __init__(self, args, Model, config, num_labels=2):\n",
    "        \"\"\"Initialize the model\"\"\"\n",
    "        super(AutoModelForTokenClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = Model\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(args.drop_ratio)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self._init_weights(self.classifier)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        target_mask,\n",
    "        token_type_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        head_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary\n",
    "            `target_mask`: a torch.LongTensor of shape [batch_size, sequence_length] with the mask for target wor. 1 for target word and 0 otherwise.\n",
    "            `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices\n",
    "                selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n",
    "            `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1].\n",
    "                It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch.\n",
    "                It's the mask that we typically use for attention when a batch has varying length sentences.\n",
    "            `labels`: optional labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n",
    "                with indices selected in [0, ..., num_labels].\n",
    "            `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
    "                It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
    "        \"\"\"\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "        )\n",
    "        sequence_output = outputs[0]  # [batch, max_len, hidden]\n",
    "        target_output = sequence_output * target_mask.unsqueeze(2)\n",
    "        target_output = self.dropout(target_output)\n",
    "        target_output = target_output.sum(1) / target_mask.sum()  # [batch, hideen]\n",
    "\n",
    "        logits = self.classifier(target_output)\n",
    "        logits = self.logsoftmax(logits)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.NLLLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        return logits\n",
    "\n",
    "\n",
    "class AutoModelForSequenceClassification_SPV(nn.Module):\n",
    "    \"\"\"MelBERT with only SPV\"\"\"\n",
    "\n",
    "    def __init__(self, args, Model, config, num_labels=2):\n",
    "        \"\"\"Initialize the model\"\"\"\n",
    "        super(AutoModelForSequenceClassification_SPV, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.encoder = Model\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(args.drop_ratio)\n",
    "        self.classifier = nn.Linear(config.hidden_size * 2, num_labels)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self._init_weights(self.classifier)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        target_mask,\n",
    "        token_type_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        head_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary\n",
    "            `target_mask`: a torch.LongTensor of shape [batch_size, sequence_length] with the mask for target wor. 1 for target word and 0 otherwise.\n",
    "            `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices\n",
    "                selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n",
    "            `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1].\n",
    "            `labels`: optional labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n",
    "                with indices selected in [0, ..., num_labels].\n",
    "            `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
    "                It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
    "        \"\"\"\n",
    "        outputs = self.encoder(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "        )\n",
    "        sequence_output = outputs[0]  # [batch, max_len, hidden]\n",
    "        pooled_output = outputs[1]  # [batch, hidden]\n",
    "\n",
    "        # Get target ouput with target mask\n",
    "        target_output = sequence_output * target_mask.unsqueeze(2)  # [batch, hidden]\n",
    "\n",
    "        # dropout\n",
    "        target_output = self.dropout(target_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Get mean value of target output if the target output consistst of more than one token\n",
    "        target_output = target_output.mean(1)\n",
    "\n",
    "        logits = self.classifier(torch.cat([target_output, pooled_output], dim=1))\n",
    "        logits = self.logsoftmax(logits)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.NLLLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        return logits\n",
    "\n",
    "\n",
    "class AutoModelForSequenceClassification_MIP(nn.Module):\n",
    "    \"\"\"MelBERT with only MIP\"\"\"\n",
    "\n",
    "    def __init__(self, args, Model, config, num_labels=2):\n",
    "        \"\"\"Initialize the model\"\"\"\n",
    "        super(AutoModelForSequenceClassification_MIP, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.encoder = Model\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(args.drop_ratio)\n",
    "        self.args = args\n",
    "        self.classifier = nn.Linear(config.hidden_size * 2, num_labels)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self._init_weights(self.classifier)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        input_ids_2,\n",
    "        target_mask,\n",
    "        target_mask_2,\n",
    "        attention_mask_2,\n",
    "        token_type_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        head_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] with the first input token indices in the vocabulary\n",
    "            `input_ids_2`: a torch.LongTensor of shape [batch_size, sequence_length] with the second input token indicies\n",
    "            `target_mask`: a torch.LongTensor of shape [batch_size, sequence_length] with the mask for target word in the first input. 1 for target word and 0 otherwise.\n",
    "            `target_mask_2`: a torch.LongTensor of shape [batch_size, sequence_length] with the mask for target word in the second input. 1 for target word and 0 otherwise.\n",
    "            `attention_mask_2`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1] for the second input.\n",
    "            `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices\n",
    "                selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n",
    "            `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1] for the first input.\n",
    "            `labels`: optional labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n",
    "                with indices selected in [0, ..., num_labels].\n",
    "            `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
    "                It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
    "        \"\"\"\n",
    "        # First encoder for full sentence\n",
    "        outputs = self.encoder(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "        )\n",
    "        sequence_output = outputs[0]  # [batch, max_len, hidden]\n",
    "\n",
    "        # Get target ouput with target mask\n",
    "        target_output = sequence_output * target_mask.unsqueeze(2)\n",
    "        target_output = self.dropout(target_output)\n",
    "        target_output = target_output.sum(1) / target_mask.sum()  # [batch, hidden]\n",
    "\n",
    "        # Second encoder for only the target word\n",
    "        outputs_2 = self.encoder(input_ids_2, attention_mask=attention_mask_2, head_mask=head_mask)\n",
    "        sequence_output_2 = outputs_2[0]  # [batch, max_len, hidden]\n",
    "\n",
    "        # Get target ouput with target mask\n",
    "        target_output_2 = sequence_output_2 * target_mask_2.unsqueeze(2)\n",
    "        target_output_2 = self.dropout(target_output_2)\n",
    "        target_output_2 = target_output_2.sum(1) / target_mask_2.sum()\n",
    "\n",
    "        logits = self.classifier(torch.cat([target_output_2, target_output], dim=1))\n",
    "        logits = self.logsoftmax(logits)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.NLLLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        return logits\n",
    "\n",
    "\n",
    "class AutoModelForSequenceClassification_SPV_MIP(nn.Module):\n",
    "    \"\"\"MelBERT\"\"\"\n",
    "\n",
    "    def __init__(self, args, Model, config, num_labels=2):\n",
    "        \"\"\"Initialize the model\"\"\"\n",
    "        super(AutoModelForSequenceClassification_SPV_MIP, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.encoder = Model\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(args.drop_ratio)\n",
    "        self.args = args\n",
    "\n",
    "        self.SPV_linear = nn.Linear(config.hidden_size * 2, args.classifier_hidden)\n",
    "        self.MIP_linear = nn.Linear(config.hidden_size * 2, args.classifier_hidden)\n",
    "        self.classifier = nn.Linear(args.classifier_hidden * 2, num_labels)\n",
    "        self._init_weights(self.SPV_linear)\n",
    "        self._init_weights(self.MIP_linear)\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self._init_weights(self.classifier)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        input_ids_2,\n",
    "        target_mask,\n",
    "        target_mask_2,\n",
    "        attention_mask_2,\n",
    "        token_type_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        head_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] with the first input token indices in the vocabulary\n",
    "            `input_ids_2`: a torch.LongTensor of shape [batch_size, sequence_length] with the second input token indicies\n",
    "            `target_mask`: a torch.LongTensor of shape [batch_size, sequence_length] with the mask for target word in the first input. 1 for target word and 0 otherwise.\n",
    "            `target_mask_2`: a torch.LongTensor of shape [batch_size, sequence_length] with the mask for target word in the second input. 1 for target word and 0 otherwise.\n",
    "            `attention_mask_2`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1] for the second input.\n",
    "            `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices\n",
    "                selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n",
    "            `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1] for the first input.\n",
    "            `labels`: optional labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n",
    "                with indices selected in [0, ..., num_labels].\n",
    "            `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
    "                It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
    "        \"\"\"\n",
    "        # First encoder for full sentence\n",
    "        outputs = self.encoder(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "        )\n",
    "        sequence_output = outputs[0]  # [batch, max_len, hidden]\n",
    "        pooled_output = outputs[1]  # [batch, hidden]\n",
    "\n",
    "        # Get target ouput with target mask\n",
    "        target_output = sequence_output * target_mask.unsqueeze(2)\n",
    "\n",
    "        # dropout\n",
    "        target_output = self.dropout(target_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        target_output = target_output.mean(1)  # [batch, hidden]\n",
    "\n",
    "        # Second encoder for only the target word\n",
    "        outputs_2 = self.encoder(input_ids_2, attention_mask=attention_mask_2, head_mask=head_mask)\n",
    "        sequence_output_2 = outputs_2[0]  # [batch, max_len, hidden]\n",
    "\n",
    "        # Get target ouput with target mask\n",
    "        target_output_2 = sequence_output_2 * target_mask_2.unsqueeze(2)\n",
    "        target_output_2 = self.dropout(target_output_2)\n",
    "        target_output_2 = target_output_2.mean(1)\n",
    "\n",
    "        # Get hidden vectors each from SPV and MIP linear layers\n",
    "        SPV_hidden = self.SPV_linear(torch.cat([pooled_output, target_output], dim=1))\n",
    "        MIP_hidden = self.MIP_linear(torch.cat([target_output_2, target_output], dim=1))\n",
    "\n",
    "        logits = self.classifier(self.dropout(torch.cat([SPV_hidden, MIP_hidden], dim=1)))\n",
    "        logits = self.logsoftmax(logits)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.NLLLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUJOIxvv9fR0"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-30T17:59:30.011130Z",
     "iopub.status.busy": "2025-05-30T17:59:30.010805Z"
    },
    "id": "Oiw0kaRn9igI",
    "outputId": "bf94604b-35cb-4276-c464-cae78004cc77"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "'''\n",
    "from utils import Config, Logger, make_log_dir\n",
    "from modeling import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification_SPV,\n",
    "    AutoModelForSequenceClassification_MIP,\n",
    "    AutoModelForSequenceClassification_SPV_MIP,\n",
    ")\n",
    "from run_classifier_dataset_utils import processors, output_modes, compute_metrics\n",
    "from data_loader import load_train_data, load_train_data_kf, load_test_data\n",
    "'''\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
    "ARGS_NAME = \"training_args.bin\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    # read configs\n",
    "    config = Config(main_conf_path=\"./\")\n",
    "\n",
    "    # apply system arguments if exist\n",
    "    argv = sys.argv[1:]\n",
    "    if len(argv) > 0:\n",
    "        cmd_arg = OrderedDict()\n",
    "        argvs = \" \".join(sys.argv[1:]).split(\" \")\n",
    "        for i in range(0, len(argvs), 2):\n",
    "            arg_name, arg_value = argvs[i], argvs[i + 1]\n",
    "            arg_name = arg_name.strip(\"-\")\n",
    "            cmd_arg[arg_name] = arg_value\n",
    "        config.update_params(cmd_arg)\n",
    "\n",
    "    args = config\n",
    "    print(args.__dict__)\n",
    "\n",
    "    # logger\n",
    "    if \"saves\" in args.bert_model_save:\n",
    "        log_dir = args.bert_model_save\n",
    "        logger = Logger(log_dir)\n",
    "        config = Config(main_conf_path=log_dir)\n",
    "        old_args = copy.deepcopy(args)\n",
    "        args.__dict__.update(config.__dict__)\n",
    "\n",
    "        args.bert_model_save = old_args.bert_model_save\n",
    "        args.do_train = old_args.do_train\n",
    "        args.data_dir = old_args.data_dir\n",
    "        args.task_name = old_args.task_name\n",
    "\n",
    "        # apply system arguments if exist\n",
    "        argv = sys.argv[1:]\n",
    "        if len(argv) > 0:\n",
    "            cmd_arg = OrderedDict()\n",
    "            argvs = \" \".join(sys.argv[1:]).split(\" \")\n",
    "            for i in range(0, len(argvs), 2):\n",
    "                arg_name, arg_value = argvs[i], argvs[i + 1]\n",
    "                arg_name = arg_name.strip(\"-\")\n",
    "                cmd_arg[arg_name] = arg_value\n",
    "            config.update_params(cmd_arg)\n",
    "    else:\n",
    "        if not os.path.exists(\"saves\"):\n",
    "            os.mkdir(\"saves\")\n",
    "        log_dir = make_log_dir(os.path.join(\"saves\", args.bert_model_save))\n",
    "        logger = Logger(log_dir)\n",
    "        config.save(log_dir)\n",
    "    args.log_dir = log_dir\n",
    "\n",
    "    # set CUDA devices\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    logger.info(\"device: {} n_gpu: {}\".format(device, args.n_gpu))\n",
    "    \n",
    "    # set seed\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "           \n",
    "    # get dataset and processor\n",
    "    task_name = args.task_name.lower()\n",
    "    processor = processors[task_name]()\n",
    "    output_mode = output_modes[task_name]\n",
    "    label_list = processor.get_labels()\n",
    "    args.num_labels = len(label_list)\n",
    "\n",
    "    # build tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "    model = load_pretrained_model(args) \n",
    "\n",
    "    ########### Training ###########\n",
    "\n",
    "    # VUA18 / VUA20 for bagging\n",
    "    if args.do_train and args.task_name == \"vua\" and args.num_bagging:\n",
    "        train_data, gkf = load_train_data_kf(args, logger, processor, task_name, label_list, tokenizer, output_mode)\n",
    "\n",
    "        for fold, (train_idx, valid_idx) in enumerate(tqdm(gkf, desc=\"bagging...\")):\n",
    "            if fold != args.bagging_index:\n",
    "                continue\n",
    "\n",
    "            print(f\"bagging_index = {args.bagging_index}\")\n",
    "\n",
    "            # Load data\n",
    "            temp_train_data = TensorDataset(*train_data[train_idx])\n",
    "            train_sampler = RandomSampler(temp_train_data)\n",
    "            train_dataloader = DataLoader(temp_train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "            # Reset Model\n",
    "            model = load_pretrained_model(args)\n",
    "            model, best_result = run_train(args, logger, model, train_dataloader, processor, task_name, label_list, tokenizer, output_mode)\n",
    "\n",
    "            # Test\n",
    "            all_guids, eval_dataloader = load_test_data(args, logger, processor, task_name, label_list, tokenizer, output_mode)\n",
    "            preds = run_eval(args, logger, model, eval_dataloader, all_guids, task_name, return_preds=True)\n",
    "            with open(os.path.join(args.data_dir, f\"seed{args.seed}_preds_{fold}.p\"), \"wb\") as f:\n",
    "                pickle.dump(preds, f)\n",
    "\n",
    "            # If train data is VUA20, the model needs to be tested on VUAverb as well.\n",
    "            # You can just adjust the names of data_dir in conditions below for your own data directories.\n",
    "            if \"VUA20\" in args.data_dir:\n",
    "                # Verb\n",
    "                args.data_dir = \"data/VUAverb\"\n",
    "                all_guids, eval_dataloader = load_test_data(args, logger, processor, task_name, label_list, tokenizer, output_mode)\n",
    "                preds = run_eval(args, logger, model, eval_dataloader, all_guids, task_name, return_preds=True)\n",
    "                with open(os.path.join(args.data_dir, f\"seed{args.seed}_preds_{fold}.p\"), \"wb\") as f:\n",
    "                    pickle.dump(preds, f)\n",
    "\n",
    "            logger.info(f\"Saved to {logger.log_dir}\")\n",
    "        return\n",
    "    \n",
    "    # VUA18 / VUA20\n",
    "    if args.do_train and args.task_name == \"vua\":\n",
    "        train_dataloader = load_train_data(\n",
    "            args, logger, processor, task_name, label_list, tokenizer, output_mode\n",
    "        )\n",
    "        model, best_result = run_train(\n",
    "            args,\n",
    "            logger,\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            processor,\n",
    "            task_name,\n",
    "            label_list,\n",
    "            tokenizer,\n",
    "            output_mode,\n",
    "        )\n",
    "    # TroFi / MOH-X (K-fold)\n",
    "    elif args.do_train and args.task_name == \"trofi\":\n",
    "        k_result = []\n",
    "        for k in tqdm(range(args.kfold), desc=\"K-fold\"):\n",
    "            model = load_pretrained_model(args)\n",
    "            train_dataloader = load_train_data(\n",
    "                args, logger, processor, task_name, label_list, tokenizer, output_mode, k\n",
    "            )\n",
    "            model, best_result = run_train(\n",
    "                args,\n",
    "                logger,\n",
    "                model,\n",
    "                train_dataloader,\n",
    "                processor,\n",
    "                task_name,\n",
    "                label_list,\n",
    "                tokenizer,\n",
    "                output_mode,\n",
    "                k,\n",
    "            )\n",
    "            k_result.append(best_result)\n",
    "\n",
    "        # Calculate average result\n",
    "        avg_result = copy.deepcopy(k_result[0])\n",
    "        for result in k_result[1:]:\n",
    "            for k, v in result.items():\n",
    "                avg_result[k] += v\n",
    "        for k, v in avg_result.items():\n",
    "            avg_result[k] /= len(k_result)\n",
    "\n",
    "        logger.info(f\"-----Averge Result-----\")\n",
    "        for key in sorted(avg_result.keys()):\n",
    "            logger.info(f\"  {key} = {str(avg_result[key])}\")\n",
    "\n",
    "    # Load trained model\n",
    "    if \"saves\" in args.bert_model:\n",
    "        model = load_trained_model(args, model, tokenizer)\n",
    "\n",
    "    ########### Inference ###########\n",
    "    # VUA18 / VUA20\n",
    "    if (args.do_eval or args.do_test) and task_name == \"vua\":\n",
    "        # if test data is genre or POS tag data\n",
    "        if (\"genre\" in args.data_dir) or (\"pos\" in args.data_dir):\n",
    "            if \"genre\" in args.data_dir:\n",
    "                targets = [\"acad\", \"conv\", \"fict\", \"news\"]\n",
    "            elif \"pos\" in args.data_dir:\n",
    "                targets = [\"adj\", \"adv\", \"noun\", \"verb\", \"aux\", \"pron\"]\n",
    "            orig_data_dir = args.data_dir\n",
    "            for idx, target in tqdm(enumerate(targets)):\n",
    "                logger.info(f\"====================== Evaluating {target} =====================\")\n",
    "                args.data_dir = os.path.join(orig_data_dir, target)\n",
    "                all_guids, eval_dataloader = load_test_data(\n",
    "                    args, logger, processor, task_name, label_list, tokenizer, output_mode\n",
    "                )\n",
    "                run_eval(args, logger, model, eval_dataloader, all_guids, task_name)\n",
    "        else:\n",
    "            all_guids, eval_dataloader = load_test_data(\n",
    "                args, logger, processor, task_name, label_list, tokenizer, output_mode\n",
    "            )\n",
    "            run_eval(args, logger, model, eval_dataloader, all_guids, task_name)\n",
    "\n",
    "    # TroFi / MOH-X (K-fold)\n",
    "    elif (args.do_eval or args.do_test) and args.task_name == \"trofi\":\n",
    "        logger.info(f\"***** Evaluating with {args.data_dir}\")\n",
    "        k_result = []\n",
    "        for k in tqdm(range(10), desc=\"K-fold\"):\n",
    "            all_guids, eval_dataloader = load_test_data(\n",
    "                args, logger, processor, task_name, label_list, tokenizer, output_mode, k\n",
    "            )\n",
    "            result = run_eval(args, logger, model, eval_dataloader, all_guids, task_name)\n",
    "            k_result.append(result)\n",
    "\n",
    "        # Calculate average result\n",
    "        avg_result = copy.deepcopy(k_result[0])\n",
    "        for result in k_result[1:]:\n",
    "            for k, v in result.items():\n",
    "                avg_result[k] += v\n",
    "        for k, v in avg_result.items():\n",
    "            avg_result[k] /= len(k_result)\n",
    "\n",
    "        logger.info(f\"-----Averge Result-----\")\n",
    "        for key in sorted(avg_result.keys()):\n",
    "            logger.info(f\"  {key} = {str(avg_result[key])}\")\n",
    "    logger.info(f\"Saved to {logger.log_dir}\")\n",
    "\n",
    "\n",
    "def run_train(\n",
    "    args,\n",
    "    logger,\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    processor,\n",
    "    task_name,\n",
    "    label_list,\n",
    "    tokenizer,\n",
    "    output_mode,\n",
    "    k=None,\n",
    "):\n",
    "    tr_loss = 0\n",
    "    num_train_optimization_steps = len(train_dataloader) * args.num_train_epoch\n",
    "\n",
    "    # Prepare optimizer, scheduler\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "    if args.lr_schedule != False or args.lr_schedule.lower() != \"none\":\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(args.warmup_epoch * len(train_dataloader)),\n",
    "            num_training_steps=num_train_optimization_steps,\n",
    "        )\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Batch size = {args.train_batch_size}\")\n",
    "    logger.info(f\"  Num steps = { num_train_optimization_steps}\")\n",
    "\n",
    "    # Run training\n",
    "    model.train()\n",
    "    max_val_f1 = -1\n",
    "    max_result = {}\n",
    "    for epoch in trange(int(args.num_train_epoch), desc=\"Epoch\"):\n",
    "        tr_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "            # move batch data to gpu\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            if args.model_type in [\"MELBERT_MIP\", \"MELBERT\"]:\n",
    "                (\n",
    "                    input_ids,\n",
    "                    input_mask,\n",
    "                    segment_ids,\n",
    "                    label_ids,\n",
    "                    input_ids_2,\n",
    "                    input_mask_2,\n",
    "                    segment_ids_2,\n",
    "                ) = batch\n",
    "            else:\n",
    "                input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "            # compute loss values\n",
    "            if args.model_type in [\"BERT_SEQ\", \"BERT_BASE\", \"MELBERT_SPV\"]:\n",
    "                logits = model(\n",
    "                    input_ids,\n",
    "                    target_mask=(segment_ids == 1),\n",
    "                    token_type_ids=segment_ids,\n",
    "                    attention_mask=input_mask,\n",
    "                )\n",
    "                loss_fct = nn.NLLLoss(weight=torch.Tensor([1, args.class_weight]).to(args.device))\n",
    "                loss = loss_fct(logits.view(-1, args.num_labels), label_ids.view(-1))\n",
    "            elif args.model_type in [\"MELBERT_MIP\", \"MELBERT\"]:\n",
    "                logits = model(\n",
    "                    input_ids,\n",
    "                    input_ids_2,\n",
    "                    target_mask=(segment_ids == 1),\n",
    "                    target_mask_2=segment_ids_2,\n",
    "                    attention_mask_2=input_mask_2,\n",
    "                    token_type_ids=segment_ids,\n",
    "                    attention_mask=input_mask,\n",
    "                )\n",
    "                loss_fct = nn.NLLLoss(weight=torch.Tensor([1, args.class_weight]).to(args.device))\n",
    "                loss = loss_fct(logits.view(-1, args.num_labels), label_ids.view(-1))\n",
    "\n",
    "            # average loss if on multi-gpu.\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            if args.lr_schedule != False or args.lr_schedule.lower() != \"none\":\n",
    "                scheduler.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        logger.info(f\"[epoch {epoch+1}] ,lr: {cur_lr} ,tr_loss: {tr_loss}\")\n",
    "\n",
    "        # evaluate\n",
    "        if args.do_eval:\n",
    "            all_guids, eval_dataloader = load_test_data(\n",
    "                args, logger, processor, task_name, label_list, tokenizer, output_mode, k\n",
    "            )\n",
    "            result = run_eval(args, logger, model, eval_dataloader, all_guids, task_name)\n",
    "\n",
    "            # update\n",
    "            if result[\"f1\"] > max_val_f1:\n",
    "                max_val_f1 = result[\"f1\"]\n",
    "                max_result = result\n",
    "                if args.task_name == \"trofi\":\n",
    "                    save_model(args, model, tokenizer)\n",
    "            if args.task_name == \"vua\":\n",
    "                save_model(args, model, tokenizer)\n",
    "\n",
    "    logger.info(f\"-----Best Result-----\")\n",
    "    for key in sorted(max_result.keys()):\n",
    "        logger.info(f\"  {key} = {str(max_result[key])}\")\n",
    "\n",
    "    return model, max_result\n",
    "\n",
    "\n",
    "def run_eval(args, logger, model, eval_dataloader, all_guids, task_name, return_preds=False):\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    preds = []\n",
    "    pred_guids = []\n",
    "    out_label_ids = None\n",
    "\n",
    "    for eval_batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        eval_batch = tuple(t.to(args.device) for t in eval_batch)\n",
    "\n",
    "        if args.model_type in [\"MELBERT_MIP\", \"MELBERT\"]:\n",
    "            (\n",
    "                input_ids,\n",
    "                input_mask,\n",
    "                segment_ids,\n",
    "                label_ids,\n",
    "                idx,\n",
    "                input_ids_2,\n",
    "                input_mask_2,\n",
    "                segment_ids_2,\n",
    "            ) = eval_batch\n",
    "        else:\n",
    "            input_ids, input_mask, segment_ids, label_ids, idx = eval_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # compute loss values\n",
    "            if args.model_type in [\"BERT_BASE\", \"BERT_SEQ\", \"MELBERT_SPV\"]:\n",
    "                logits = model(\n",
    "                    input_ids,\n",
    "                    target_mask=(segment_ids == 1),\n",
    "                    token_type_ids=segment_ids,\n",
    "                    attention_mask=input_mask,\n",
    "                )\n",
    "                loss_fct = nn.NLLLoss()\n",
    "                tmp_eval_loss = loss_fct(logits.view(-1, args.num_labels), label_ids.view(-1))\n",
    "                eval_loss += tmp_eval_loss.mean().item()\n",
    "                nb_eval_steps += 1\n",
    "\n",
    "                if len(preds) == 0:\n",
    "                    preds.append(logits.detach().cpu().numpy())\n",
    "                    pred_guids.append([all_guids[i] for i in idx])\n",
    "                    out_label_ids = label_ids.detach().cpu().numpy()\n",
    "                else:\n",
    "                    preds[0] = np.append(preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "                    pred_guids[0].extend([all_guids[i] for i in idx])\n",
    "                    out_label_ids = np.append(\n",
    "                        out_label_ids, label_ids.detach().cpu().numpy(), axis=0\n",
    "                    )\n",
    "\n",
    "            elif args.model_type in [\"MELBERT_MIP\", \"MELBERT\"]:\n",
    "                logits = model(\n",
    "                    input_ids,\n",
    "                    input_ids_2,\n",
    "                    target_mask=(segment_ids == 1),\n",
    "                    target_mask_2=segment_ids_2,\n",
    "                    attention_mask_2=input_mask_2,\n",
    "                    token_type_ids=segment_ids,\n",
    "                    attention_mask=input_mask,\n",
    "                )\n",
    "                loss_fct = nn.NLLLoss()\n",
    "                tmp_eval_loss = loss_fct(logits.view(-1, args.num_labels), label_ids.view(-1))\n",
    "                eval_loss += tmp_eval_loss.mean().item()\n",
    "                nb_eval_steps += 1\n",
    "\n",
    "                if len(preds) == 0:\n",
    "                    preds.append(logits.detach().cpu().numpy())\n",
    "                    pred_guids.append([all_guids[i] for i in idx])\n",
    "                    out_label_ids = label_ids.detach().cpu().numpy()\n",
    "                else:\n",
    "                    preds[0] = np.append(preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "                    pred_guids[0].extend([all_guids[i] for i in idx])\n",
    "                    out_label_ids = np.append(\n",
    "                        out_label_ids, label_ids.detach().cpu().numpy(), axis=0\n",
    "                    )\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = preds[0]\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    print(\"Evaluation loss: \", eval_loss)\n",
    "    \n",
    "    # compute metrics\n",
    "    result = compute_metrics(preds, out_label_ids)\n",
    "\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(f\"  {key} = {str(result[key])}\")\n",
    "\n",
    "        \n",
    "    # Print number of model parameters\n",
    "    total_params = sum(\n",
    "\tparam.numel() for param in model.parameters())\n",
    "\n",
    "    print(total_params)\n",
    "    \n",
    "    # Save results to csv \n",
    "    \n",
    "    predicted = list(preds)\n",
    "    label = list(out_label_ids)\n",
    "\n",
    "    dict = {'predicted': predicted, 'label': label} \n",
    "    \n",
    "    df = pd.DataFrame(dict)\n",
    "    df.to_csv('preds_labels.csv')\n",
    "    df_ids = pd.DataFrame(pred_guids)\n",
    "    new_guids = df_ids.transpose()\n",
    "    new_guids.to_csv('pred_guids.csv')\n",
    "    \n",
    "    if return_preds:\n",
    "        return preds\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_pretrained_model(args):\n",
    "    # Pretrained Model\n",
    "    bert = AutoModel.from_pretrained(args.bert_model)\n",
    "    config = bert.config\n",
    "    config.type_vocab_size = 4\n",
    "    if \"albert\" in args.bert_model:\n",
    "        bert.embeddings.token_type_embeddings = nn.Embedding(\n",
    "            config.type_vocab_size, config.embedding_size\n",
    "        )\n",
    "    else:\n",
    "        bert.embeddings.token_type_embeddings = nn.Embedding(\n",
    "            config.type_vocab_size, config.hidden_size\n",
    "        )\n",
    "    bert._init_weights(bert.embeddings.token_type_embeddings)\n",
    "\n",
    "    # Additional Layers\n",
    "    if args.model_type in [\"BERT_BASE\"]:\n",
    "        model = AutoModelForSequenceClassification(\n",
    "            args=args, Model=bert, config=config, num_labels=args.num_labels\n",
    "        )\n",
    "    if args.model_type == \"BERT_SEQ\":\n",
    "        model = AutoModelForTokenClassification(\n",
    "            args=args, Model=bert, config=config, num_labels=args.num_labels\n",
    "        )\n",
    "    if args.model_type == \"MELBERT_SPV\":\n",
    "        model = AutoModelForSequenceClassification_SPV(\n",
    "            args=args, Model=bert, config=config, num_labels=args.num_labels\n",
    "        )\n",
    "    if args.model_type == \"MELBERT_MIP\":\n",
    "        model = AutoModelForSequenceClassification_MIP(\n",
    "            args=args, Model=bert, config=config, num_labels=args.num_labels\n",
    "        )\n",
    "    if args.model_type == \"MELBERT\":\n",
    "        model = AutoModelForSequenceClassification_SPV_MIP(\n",
    "            args=args, Model=bert, config=config, num_labels=args.num_labels\n",
    "        )\n",
    "\n",
    "    model.to(args.device)\n",
    "    if args.n_gpu > 1 and not args.no_cuda:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(args, model, tokenizer):\n",
    "    model_to_save = (\n",
    "        model.module if hasattr(model, \"module\") else model\n",
    "    )  # Only save the model it-self\n",
    "\n",
    "    # If we save using the predefined names, we can load using `from_pretrained`\n",
    "    output_model_file = os.path.join(args.log_dir, WEIGHTS_NAME)\n",
    "    output_config_file = os.path.join(args.log_dir, CONFIG_NAME)\n",
    "\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    model_to_save.config.to_json_file(output_config_file)\n",
    "    tokenizer.save_vocabulary(args.log_dir)\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    output_args_file = os.path.join(args.log_dir, ARGS_NAME)\n",
    "    torch.save(args, output_args_file)\n",
    "\n",
    "\n",
    "def load_trained_model(args, model, tokenizer):\n",
    "    # If we save using the predefined names, we can load using `from_pretrained`\n",
    "    output_model_file = os.path.join(args.log_dir, WEIGHTS_NAME)\n",
    "\n",
    "    if hasattr(model, \"module\"):\n",
    "        model.module.load_state_dict(torch.load(output_model_file))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(output_model_file))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate csv for error analysis\n",
    "import pandas as pd\n",
    "\n",
    "df_results = pd.read_csv('preds_labels.csv', header=0, names=['id', 'prediction', 'label'])\n",
    "df_guids = pd.read_csv('pred_guids.csv', header=0, names=['id', 'guid'])\n",
    "\n",
    "result = pd.merge(df_results, df_guids, on='id')\n",
    "result.to_csv('result.csv')\n",
    "\n",
    "df_test = pd.read_csv('data/fold_2/test.tsv', sep='\\t', header=0, names=['index', 'label', 'sentence', 'POS', 'w_index'])\n",
    "df_test['id'] = df_test.index\n",
    "\n",
    "result_merged = pd.merge(result, df_test, on='id')\n",
    "result_merged.to_csv('results/xlm-r-base/xlm-roberta-base-2-2e5.csv')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNsh9kFNLkem/IUNpP1al0R",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1Hb7dORXahNUL8SoMbU1-ctuztO4tr2Fx",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
